{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#           **AhmedSamyAhmed_IndustryClassificationTask**\n",
    "\n",
    "<hr>\n",
    "\n",
    "# The task Doumentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answering important questions about the task :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Cleaning\n",
    "- in a data like this , cleaning the dataset will be different from just fillna or drop_duplicates like we do in traditional datasets that we deal with , As we deal with a categorical data so in this step it'll be enought to remove punctuations and make all job titles in lower case for better knowing of each word as a stable pattern in modeling step.\n",
    "\n",
    "# 2. Classifier Model\n",
    "- In this step you are free to choose any model (naive Bayes , Decision tree , and so on...) But now i'll use (XGBoost)\n",
    "- XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.\n",
    "- The name xgboost, though, actually refers to the engineering goal to push the limit of computations resources for boosted tree algorithms. Which is the reason why many people use xgboost.\n",
    "- The three reasons to use XGBoost are also the two goals of the project:\n",
    "  1. Execution Speed.\n",
    "  2. Model Performance.\n",
    "  3. And it's a Kaggle competition winners recommendation for the above two reasons.\n",
    "  \n",
    "# 3. Dataset is imbalanced\n",
    "- In the notebook , i showed by visualization or even numbers that data in not balanced , so we have to handle this issue.\n",
    "- **Important note** When i used the dataset without being balanced , the model made overfitting ,But after data have been made to be balanced specifically using oversampling , model got give better performance .\n",
    "- Data oversampling i solved the imbalanced data problem using a famous way which's called oversampling , in this way we repeats rows of classes which is little to make all classes with the same number of rows (this Differentiates my model in next steps )\n",
    "\n",
    "# 4. Getting better performance\n",
    "- About how i can extend the model to have better performance \n",
    "  1. i can search for better hyper parameters tuning for better performance .\n",
    "  2. increase the amount of training data ,it'll help the model fit better.\n",
    "  3. i can use cross validation rather than traditional train test split.\n",
    "  4. i also can use automated Ml way to select better model.\n",
    "  \n",
    "  \n",
    "# 5. Model Evalution \n",
    "- for classification there are many model performance metrics (Percision , recall , F1 score , traditional accuracy and many other metrics) , but it depends on knowing the nature of your data well. As there is data is conserned with percision and other is conserned with recall\n",
    "- i have to keep in mind also if the model has overfitting or underfitting it affects my model performance hard.\n",
    "\n",
    "# 6. The limitations of my methodology\n",
    "- The only probelm i faced when dealing whit this dataset was when using the model without oversampling , i got a very high overfitting in my model . Other than this problem my i got very good Results .\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
